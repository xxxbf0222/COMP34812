{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "707924c3-e183-4d45-89bc-1fa7709e8241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from ClaimEvicenceDataset import ClaimEvidenceLstmDataset\n",
    "from SimpleTokenizer import SimpleTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51f2bbd3-205c-4e2c-a266-ff9325c3f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_label_distribution(dataset):\n",
    "    label_distribution = [0,0]\n",
    "    for label in dataset[\"label\"]:\n",
    "        label_distribution[label] += 1\n",
    "    return label_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96f83544-db8f-4c2f-a897-d175083cccf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data distribution: [15654, 5854]\n",
      "Weight for neg/pos class: [1.37396193 3.67406901]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pd.read_csv(\"./data/train.csv\")\n",
    "\n",
    "train_label_distrib = calc_label_distribution(train_dataset)\n",
    "print(\"Training data distribution:\",train_label_distrib)\n",
    "total = np.sum(train_label_distrib)\n",
    "class_weights = total/train_label_distrib\n",
    "print(\"Weight for neg/pos class:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1fe2877b-9dc1-4ed4-b88b-be41667db597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development data distribution: [4286, 1640]\n",
      "Uniformed development data distribution: [1640, 1640]\n"
     ]
    }
   ],
   "source": [
    "dev_dataset = pd.read_csv(\"./data/dev.csv\")\n",
    "dev_label_distrib = calc_label_distribution(dev_dataset)\n",
    "print(\"Development data distribution:\",dev_label_distrib)\n",
    "\n",
    "selected_idx = []\n",
    "num_neg_labels = 0\n",
    "idx = 0\n",
    "for label in dev_dataset[\"label\"]:\n",
    "    if label == 1:\n",
    "        selected_idx.append(idx)\n",
    "    else:\n",
    "        if num_neg_labels < dev_label_distrib[1]:\n",
    "            selected_idx.append(idx)\n",
    "            num_neg_labels += 1\n",
    "    idx += 1\n",
    "uniform_dev_dataset = dev_dataset.iloc[selected_idx]\n",
    "uniformed_dev_label_distrib = calc_label_distribution(uniform_dev_dataset)\n",
    "print(\"Uniformed development data distribution:\",uniformed_dev_label_distrib)\n",
    "\n",
    "uniform_dev_dataset.to_csv(\"./data/dev_uniformed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477d2c7-09b8-42d9-87fc-e4a139a02c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dac27c1f-1274-4b85-82ae-40ebb3f548c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading Training Set from file, using simple tokenizer:\n",
      "Loading 'word2vec-google-news-300' as pretrained word embeddings...\n",
      "Loading training dataset and building vocab...  row (21507/21508)\n",
      "Building Word Embeddings...\n",
      "Converting Text to Sequences...\n",
      "Done!\n",
      "Dataset saved to: ./data/pt/train_dataset.pt\n",
      "Word Embedding Matrix saved to: ./data/pt/default_embedding_mat.pt\n",
      "Vocab saved to: ./data/default_vocab.pt\n",
      "\n",
      "==> Loading Development Set from file:\n",
      "Loading validation dataset...   row (5925/5926)\n",
      "Converting Text to Sequences...\n",
      "Using exist vocab...\n",
      "Done!\n",
      "Dataset saved to: ./data/pt/dev_dataset.pt\n",
      "\n",
      "==> Loading Uniformed Development Set from file:\n",
      "Loading validation dataset...   row (3279/3280)\n",
      "Converting Text to Sequences...\n",
      "Using exist vocab...\n",
      "Done!\n",
      "Dataset saved to: ./data/dev_uniformed_dataset.pt\n",
      "==> Loading Training Set from file, using POS tokenizer:\n",
      "Loading 'word2vec-google-news-300' as pretrained word embeddings...\n",
      "Loading training dataset and building vocab...  row (21507/21508)\n",
      "Building Word Embeddings...\n",
      "Converting Text to Sequences...\n",
      "Done!\n",
      "Dataset saved to: ./data/pt/pos_train_dataset.pt\n",
      "Word Embedding Matrix saved to: ./data/pt/pos_embedding_mat.pt\n",
      "Vocab saved to: ./data/pos_vocab.pt\n",
      "\n",
      "==> Loading Development Set from file:\n",
      "Loading validation dataset...   row (5925/5926)\n",
      "Converting Text to Sequences...\n",
      "Using exist vocab...\n",
      "Done!\n",
      "Dataset saved to: ./data/pt/pos_dev_dataset.pt\n",
      "\n",
      "==> Loading Uniformed Development Set from file:\n",
      "Loading validation dataset...   row (3279/3280)\n",
      "Converting Text to Sequences...\n",
      "Using exist vocab...\n",
      "Done!\n",
      "Dataset saved to: ./data/pt/pos_dev_uniformed_dataset.pt\n"
     ]
    }
   ],
   "source": [
    "print(\"==> Loading Training Set from file, using simple tokenizer:\")\n",
    "train_dataset = ClaimEvidenceLstmDataset(\"./data/train.csv\", type=\"train\", tokenizer=SimpleTokenizer(to_lower=False, keep_punctuation=False))\n",
    "train_dataset.save_dataset(\"./data/pt/train_dataset.pt\")\n",
    "train_dataset.save_embedding_mat(\"./data/pt/default_embedding_mat.pt\")\n",
    "train_dataset.save_vocab(\"./data/default_vocab.pt\")\n",
    "\n",
    "print(\"\\n==> Loading Development Set from file:\")\n",
    "val_dataset = ClaimEvidenceLstmDataset(\"./data/dev.csv\",\n",
    "                                   vocab=train_dataset.vocab,\n",
    "                                   type=\"validation\")\n",
    "val_dataset.save_dataset(\"./data/pt/dev_dataset.pt\")\n",
    "\n",
    "print(\"\\n==> Loading Uniformed Development Set from file:\")\n",
    "val_dataset = ClaimEvidenceLstmDataset(\"./data/dev_uniformed.csv\",\n",
    "                                   vocab=train_dataset.vocab,\n",
    "                                   type=\"validation\")\n",
    "val_dataset.save_dataset(\"./data/dev_uniformed_dataset.pt\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\n==> Loading Training Set from file, using POS tokenizer:\")\n",
    "train_dataset = ClaimEvidenceLstmDataset(\"./data/train.csv\", type=\"train\")\n",
    "train_dataset.save_dataset(\"./data/pt/pos_train_dataset.pt\")\n",
    "train_dataset.save_embedding_mat(\"./data/pt/pos_embedding_mat.pt\")\n",
    "train_dataset.save_vocab(\"./data/pos_vocab.pt\")\n",
    "\n",
    "print(\"\\n==> Loading Development Set from file:\")\n",
    "val_dataset = ClaimEvidenceLstmDataset(\"./data/dev.csv\",\n",
    "                                   vocab=train_dataset.vocab,\n",
    "                                   type=\"validation\")\n",
    "val_dataset.save_dataset(\"./data/pt/pos_dev_dataset.pt\")\n",
    "\n",
    "print(\"\\n==> Loading Uniformed Development Set from file:\")\n",
    "val_dataset = ClaimEvidenceLstmDataset(\"./data/dev_uniformed.csv\",\n",
    "                                   vocab=train_dataset.vocab,\n",
    "                                   type=\"validation\")\n",
    "val_dataset.save_dataset(\"./data/pt/pos_dev_uniformed_dataset.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
